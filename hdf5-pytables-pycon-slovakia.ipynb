{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with HDF5 and PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*11/03/2018 - Giacomo Debidda @PyCon Slovakia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.18'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.hdf5_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "PyTables version:    3.4.2\n",
      "HDF5 version:        1.8.18\n",
      "NumPy version:       1.14.1\n",
      "Numexpr version:     2.6.4 (not using Intel's VML/MKL)\n",
      "Zlib version:        1.2.11 (in Python interpreter)\n",
      "LZO version:         2.09 (Feb 04 2015)\n",
      "BZIP2 version:       1.0.6 (6-Sept-2010)\n",
      "Blosc version:       1.11.3 (2017-03-09)\n",
      "Blosc compressors:   blosclz (1.0.5), lz4 (1.7.5), lz4hc (1.7.5), snappy (1.1.1), zlib (1.2.8), zstd (1.1.3)\n",
      "Blosc filters:       shuffle, bitshuffle\n",
      "Python version:      3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42) \n",
      "[GCC 7.2.0]\n",
      "Platform:            Linux-4.4.0-116-generic-x86_64-with-debian-stretch-sid\n",
      "Byte-ordering:       little\n",
      "Detected cores:      4\n",
      "Default encoding:    utf-8\n",
      "Default FS encoding: utf-8\n",
      "Default locale:      (en_US, UTF-8)\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
     ]
    }
   ],
   "source": [
    "tb.print_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jack/Repos/hdf5-pycon-slovakia/data\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5: a filesystem in a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 is a data model, library, and file format for storing and managing big and complex data.\n",
    "\n",
    "An HDF5 file can be thought of as a container (or group) that holds a variety of heterogeneous data objects (or datasets). The datasets can be almost anything: images, tables, graphs, or even documents, such as PDF or Excel.\n",
    "\n",
    "- Datasets (i.e. files in a filesystem)\n",
    "- Groups (i.e. directories in a filesystem)\n",
    "- Attributes (i.e. metadata of file/directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HDF5 structure](img/hdf5_structure.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with groups and group members is similar to working with directories and files in UNIX.\n",
    "\n",
    "**/** root group (every HDF5 file has a root group)\n",
    "\n",
    "**/foo** member of the root group called foo\n",
    "\n",
    "**/foo/bar** member of the group foo called bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 in the Python data stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![h5py - PyTables refactor](img/h5py-pytables-refactor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTables logo](img/pytables-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does not want to be a complete wrapper for the entire HDF5 C API\n",
    "- High level abstraction over HDF5 (it's more \"battery included\" than h5py)\n",
    "- Does not depend on h5py (at the moment)\n",
    "- Natural naming\n",
    "- Fast searches (indexing, out-of-core querying)\n",
    "- Built-in compression\n",
    "- Undo mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTables provides high-level abstractions over the HDF5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogenous dataset:\n",
    "\n",
    "- **Array**\n",
    "- **CArray**\n",
    "- **EArray**\n",
    "- **VLArray**\n",
    "\n",
    "Heterogenous dataset:\n",
    "\n",
    "- **Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(10).astype('float32')\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural naming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables nodes (i.e. datasets and groups in the HDF5 file) can be accessed with the dot notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/.virtualenvs/hdf5-pycon-slovakia/lib/python3.6/site-packages/tables/path.py:112: NaturalNameWarning: object name is not a valid Python identifier: 'my array'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  NaturalNameWarning)\n"
     ]
    }
   ],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_array(where='/', \n",
    "                   name='my array',\n",
    "                   title='My PyTables Array',\n",
    "                   obj=arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-array-class)\n",
    "\n",
    "- Fastest I/O speed\n",
    "- Must fit in memory\n",
    "- Not compressible\n",
    "- Not enlargeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_array(where='/', \n",
    "                   name='my_array',\n",
    "                   title='My PyTables Array',\n",
    "                   obj=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/my_array (Array(4,)) 'My PyTables Array'\n"
     ]
    }
   ],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f:\n",
    "    print(f.root.my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\r\n",
      "/my_array (Array(4,)) 'My PyTables Array'\r\n"
     ]
    }
   ],
   "source": [
    "!ptdump 'data/my_pytables_file.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n",
      "/ (RootGroup)\r\n",
      "`--my_array (Array)\r\n",
      "      mem=32.0B, disk=32.0B [100.0%]\r\n",
      "\r\n",
      "------------------------------------------------------------\r\n",
      "Total branch leaves:    1\r\n",
      "Total branch size:      32.0B in memory, 32.0B on disk\r\n",
      "Mean compression ratio: 1.00\r\n",
      "HDF5 file size:         2.2kB\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!pttree --use-si-units --sort-by 'size' 'data/my_pytables_file.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#carrayclassdescr)\n",
    "\n",
    "- Compressible, chunked storage\n",
    "- Not enlargeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = tb.Filters(complevel=5, complib='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips on how to use compression (from the PyTables docs)\n",
    "\n",
    "- A mid-level (5) compression is sufficient. No need to go all the way up (9)\n",
    "- Use zlib if you must guarantee complete portability\n",
    "- Use blosc all other times (it is optimized for HDF5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    f.create_carray(\n",
    "        where='/',\n",
    "        name='my_carray',\n",
    "        title='My PyTables CArray'\n",
    "        obj=arr,\n",
    "        filters=filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docs](http://www.pytables.org/usersguide/libref/homogenous_storage.html#earrayclassdescr)\n",
    "\n",
    "- Enlargeable on **one** dimension (append)\n",
    "- Compressible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One (and only one) of the shape dimensions *must* be 0.\n",
    "# The dimension being 0 means that the resulting EArray object can be extended along it.\n",
    "# Multiple enlargeable dimensions are not supported (at the moment).\n",
    "num_columns = 5\n",
    "shape = (0, num_columns)\n",
    "\n",
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f:\n",
    "    # you can create an EArray and fill it later, but you need to specify atom and shape\n",
    "    f.create_earray(\n",
    "        where='/',\n",
    "        name='my_earray',\n",
    "        title='My PyTables EArray',\n",
    "        atom=tb.Float32Atom(),\n",
    "        shape=shape,\n",
    "        filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 1000000  # 1 million\n",
    "matrix = np.random.random((num_rows, num_columns)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f:\n",
    "    earray = f.root.my_earray\n",
    "    earray.append(sequence=matrix[0:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f:\n",
    "    earray = f.root.my_earray\n",
    "    earray.append(sequence=matrix[11:50, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing a real dataset: NYC yellow taxi dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without a real world example I find it hard to reason about...\n",
    "http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"LocationID\",\"Borough\",\"Zone\",\"service_zone\"\n",
      "1,\"EWR\",\"Newark Airport\",\"EWR\"\n",
      "2,\"Queens\",\"Jamaica Bay\",\"Boro Zone\"\n",
      "3,\"Bronx\",\"Allerton/Pelham Gardens\",\"Boro Zone\"\n",
      "4,\"Manhattan\",\"Alphabet City\",\"Yellow Zone\"\n",
      "5,\"Staten Island\",\"Arden Heights\",\"Boro Zone\"\n",
      "6,\"Staten Island\",\"Arrochar/Fort Wadsworth\",\"Boro Zone\"\n",
      "7,\"Queens\",\"Astoria\",\"Boro Zone\"\n",
      "8,\"Queens\",\"Astoria Park\",\"Boro Zone\"\n",
      "9,\"Queens\",\"Auburndale\",\"Boro Zone\"\n",
      "10,\"Queens\",\"Baisley Park\",\"Boro Zone\"\n",
      "11,\"Brooklyn\",\"Bath Beach\",\"Boro Zone\"\n",
      "12,\"Manhattan\",\"Battery Park\",\"Yellow Zone\"\n",
      "13,\"Manhattan\",\"Battery Park City\",\"Yellow Zone\"\n",
      "14,\"Brooklyn\",\"Bay Ridge\",\"Boro Zone\"\n",
      "15,\"Queens\",\"Bay Terrace/Fort Totten\",\"Boro Zone\"\n",
      "16,\"Queens\",\"Bayside\",\"Boro Zone\"\n",
      "17,\"Brooklyn\",\"Bedford\",\"Boro Zone\"\n",
      "18,\"Bronx\",\"Bedford Park\",\"Boro Zone\"\n",
      "19,\"Queens\",\"Bellerose\",\"Boro Zone\"\n",
      "20,\"Bronx\",\"Belmont\",\"Boro Zone\"\n",
      "21,\"Brooklyn\",\"Bensonhurst East\",\"Boro Zone\"\n",
      "22,\"Brooklyn\",\"Bensonhurst West\",\"Boro Zone\"\n",
      "\u001b[K:\u001b[Ka/taxi+_zone_lookup.csv\u001b[m\u001b[K\u0007"
     ]
    }
   ],
   "source": [
    "!less 'data/taxi+_zone_lookup.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount\n",
      "\n",
      "1,2017-12-01 00:12:00,2017-12-01 00:12:51,1,.00,1,N,226,226,3,2.5,0.5,0.5,0,0,0.3,3.8\n",
      "1,2017-12-01 00:13:37,2017-12-01 00:13:47,1,.00,1,N,226,226,3,2.5,0.5,0.5,0,0,0.3,3.8\n",
      "1,2017-12-01 00:14:15,2017-12-01 00:15:05,1,.00,1,N,226,226,3,2.5,0.5,0.5,0,0,0.3,3.8\n",
      "1,2017-12-01 00:15:33,2017-12-01 00:15:37,1,.00,1,N,226,226,3,2.5,0.5,0.5,0,0,0.3,3.8\n",
      "1,2017-12-01 00:50:03,2017-12-01 00:53:35,1,.00,1,N,145,145,2,4,0.5,0.5,0,0,0.3,5.3\n",
      "1,2017-12-01 00:14:20,2017-12-01 00:28:35,1,4.20,1,N,82,258,2,15,0.5,0.5,0,0,0.3,16.3\n",
      "1,2017-12-01 00:20:32,2017-12-01 00:31:24,1,5.40,1,N,50,116,2,17,0.5,0.5,0,0,0.3,18.3\n",
      "1,2017-12-01 00:01:46,2017-12-01 00:12:19,1,1.90,1,N,161,107,1,9,0.5,0.5,2.05,0,0.3,12.35\n",
      "1,2017-12-01 00:17:52,2017-12-01 00:32:35,1,3.30,1,N,107,263,1,12.5,0.5,0.5,2.07,0,0.3,15.87\n",
      "\u001b[K:\u001b[K12-01 00:10:00,2017-12-01 00:24:35,1,2.80,1,N,264,87,1,12,0.5,0.5,2.65,0,\u001b[7mdata/yellow_tripdata_2017-12.csv\u001b[m\u001b[K\u0007"
     ]
    }
   ],
   "source": [
    "!less 'data/yellow_tripdata_2017-12.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dictionary for NY yellow taxi CSV files\n",
    "# http://www.nyc.gov/html/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\n",
    "data_dictionary = {\n",
    "    'VendorID': 'A code indicating the TPEP provider that provided the record',\n",
    "    'tpep_pickup_datetime': 'The date and time when the meter was engaged ',\n",
    "    'tpep_dropoff_datetime': 'The date and time when the meter was disengaged ',\n",
    "    'passenger_count': 'The number of passengers in the vehicle',\n",
    "    'trip_distance': 'The elapsed trip distance in miles reported by the taximeter',\n",
    "    'PULocationID': 'A code indicating the zone + borough of th pickup location',\n",
    "    'DOLocationID': 'A code indicating the zone + borough of th dropoff location',\n",
    "    'payment_type': 'A numeric code signifying how the passenger paid for the trip',\n",
    "    'fare_amount': 'The time-and-distance fare calculated by the meter',\n",
    "    'tip_amount': 'Tip amount – This field is automatically populated for credit card tips. Cash tips are not included',\n",
    "    'total_amount': 'The total amount charged to passengers. Does not include cash tips',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiTableDescription(tb.IsDescription):\n",
    "    vendor_id = tb.UInt8Col(pos=0)\n",
    "    pickup_timestamp_ms = tb.Int64Col()\n",
    "    dropoff_timestamp_ms = tb.Int64Col()\n",
    "    passenger_count = tb.UInt8Col()\n",
    "    trip_distance = tb.Float32Col()\n",
    "    pickup_location_id = tb.UInt16Col()\n",
    "    dropoff_location_id = tb.UInt16Col()\n",
    "    payment_type = tb.UInt8Col()\n",
    "    fare_amount = tb.Float32Col()\n",
    "    tip_amount = tb.Float32Col()\n",
    "    total_amount = tb.Float32Col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jack/Repos/hdf5-pycon-slovakia/data/NYC-yellow-taxis-100k.h5\n"
     ]
    }
   ],
   "source": [
    "h5_file_path = os.path.join(data_dir, 'NYC-yellow-taxis-100k.h5')\n",
    "print(h5_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = tb.Filters(complevel=5, complib='zlib')\n",
    "\n",
    "with tb.open_file(filename=h5_file_path, mode='w') as f:\n",
    "    f.create_table(\n",
    "        where='/',\n",
    "        name='yellow_taxis_2017_12',\n",
    "        description=TaxiTableDescription,\n",
    "        title='NYC Yellow Taxi data December 2017',\n",
    "        filters=filters)\n",
    "    # add metadata\n",
    "    table_where = '/yellow_taxis_2017_12'\n",
    "    for key, val in data_dictionary.items():\n",
    "        f.set_node_attr(where=table_where, attrname=key, attrvalue=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\r\n",
      "/yellow_taxis_2017_12 (Table(0,), shuffle, zlib(5)) 'NYC Yellow Taxi data December 2017'\r\n"
     ]
    }
   ],
   "source": [
    "!ptdump 'data/NYC-yellow-taxis-100k.h5'  # try also h5dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_timestamp_ms(date_obj):\n",
    "    timestamp_in_nanoseconds = date_obj.astype('int64')\n",
    "    timestamp_in_ms = (timestamp_in_nanoseconds / 1000000).astype('int64')\n",
    "    return timestamp_in_ms\n",
    "\n",
    "def fill_table(table, mapping, df):\n",
    "    num_records = df.shape[0]  # it's equal to the chunksize used in read_csv\n",
    "    row = table.row\n",
    "    for i in range(num_records):\n",
    "        row['vendor_id'] = df[mapping['vendor_id']].values[i]\n",
    "\n",
    "        pickup_ms = date_to_timestamp_ms(df[mapping['pickup_datetime']].values[i])\n",
    "        row['pickup_timestamp_ms'] = pickup_ms\n",
    "        dropoff_ms = date_to_timestamp_ms(df[mapping['dropoff_datetime']].values[i])\n",
    "        row['dropoff_timestamp_ms'] = dropoff_ms\n",
    "\n",
    "        row['passenger_count'] = df['passenger_count'].values[i]\n",
    "        row['trip_distance'] = df['trip_distance'].values[i]\n",
    "\n",
    "        row['pickup_location_id'] = df['PULocationID'].values[i]\n",
    "        row['dropoff_location_id'] = df['DOLocationID'].values[i]\n",
    "\n",
    "        row['fare_amount'] = df['fare_amount'].values[i]\n",
    "        row['tip_amount'] = df['tip_amount'].values[i]\n",
    "        row['total_amount'] = df['total_amount'].values[i]\n",
    "\n",
    "        row['payment_type'] = df['payment_type'].values[i]\n",
    "        row.append()\n",
    "    table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remember to flush:* Remember, flushing a table is a very important step as it will not only help to maintain the integrity of your file, but also will free valuable memory resources (i.e. internal buffers) that your program may need for other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 12 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Open the HDF5 file in 'a'ppend mode and populate the table with CSV data\n",
    "with tb.open_file(filename=h5_file_path, mode='a') as f:\n",
    "    # Left, the key we want to use. Right, the key in the CSV file\n",
    "    mapping = {\n",
    "        'vendor_id': 'VendorID',\n",
    "        'pickup_datetime': 'tpep_pickup_datetime',\n",
    "        'dropoff_datetime': 'tpep_dropoff_datetime',\n",
    "        'pickup_location_id': 'PULocationID',\n",
    "        'dropoff_location_id': 'DOLocationID'\n",
    "    }\n",
    "\n",
    "    # define the dtype to use when reading the CSV with pandas (this has nothing to do with the HDF5 table)\n",
    "    dtype = {'VendorID': 'category', 'payment_type': 'category'}\n",
    "    parse_dates = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "\n",
    "    table = f.get_node(where='/yellow_taxis_2017_12')\n",
    " \n",
    "    csv_file_path = os.path.join(data_dir, 'yellow_tripdata_2017-12.csv')\n",
    "\n",
    "    # read in chunks because these CSV files are too big\n",
    "    chunksize = 100000\n",
    "    for chunk in pd.read_csv(\n",
    "        csv_file_path, chunksize=chunksize, dtype=dtype,\n",
    "        skipinitialspace=True, parse_dates=parse_dates):\n",
    "        df = chunk.reset_index(drop=True)\n",
    "        fill_table(table, mapping, df)\n",
    "        # remove the break statement to process all chunks (it will take ~20 minutes)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.is_pytables_file('data/NYC-yellow-taxis-100k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method remove_rows in module tables.table:\n",
      "\n",
      "remove_rows(start=None, stop=None, step=None) method of tables.table.Table instance\n",
      "    Remove a range of rows in the table.\n",
      "    \n",
      "    If only start is supplied, that row and all following will be deleted.\n",
      "    If a range is supplied, i.e. both the start and stop parameters are\n",
      "    passed, all the rows in the range are removed.\n",
      "    \n",
      "    .. versionchanged:: 3.0\n",
      "       The start, stop and step parameters now behave like in slice.\n",
      "    \n",
      "    .. seealso:: remove_row()\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    start : int\n",
      "        Sets the starting row to be removed. It accepts negative values\n",
      "        meaning that the count starts from the end.  A value of 0 means the\n",
      "        first row.\n",
      "    stop : int\n",
      "        Sets the last row to be removed to stop-1, i.e. the end point is\n",
      "        omitted (in the Python range() tradition). Negative values are also\n",
      "        accepted. If None all rows after start will be removed.\n",
      "    step : int\n",
      "        The step size between rows to remove.\n",
      "    \n",
      "        .. versionadded:: 3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    Removing rows from 5 to 10 (excluded)::\n",
      "    \n",
      "        t.remove_rows(5, 10)\n",
      "    \n",
      "    Removing all rows starting from the 10th::\n",
      "    \n",
      "        t.remove_rows(10)\n",
      "    \n",
      "    Removing the 6th row::\n",
      "    \n",
      "        t.remove_rows(6, 7)\n",
      "    \n",
      "    .. note::\n",
      "    \n",
      "        removing a single row can be done using the specific\n",
      "        :meth:`remove_row` method.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(table.remove_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n",
      "/ (RootGroup)\r\n",
      "`--yellow_taxis_2017_12 (Table)\r\n",
      "      mem=7.8MB, disk=3.5MB [100.0%]\r\n",
      "\r\n",
      "------------------------------------------------------------\r\n",
      "Total branch leaves:    1\r\n",
      "Total branch size:      7.8MB in memory, 3.5MB on disk\r\n",
      "Mean compression ratio: 0.45\r\n",
      "HDF5 file size:         3.5MB\r\n",
      "------------------------------------------------------------\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!pttree --use-si-units --sort-by 'size' 'data/NYC-yellow-taxis-100k.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/tomkooij/scipy2017/blob/master/notebooks/03-Chunking.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/tomkooij/scipy2017/blob/master/notebooks/04-Using-Compression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/tomkooij/scipy2017/blob/master/notebooks/07-Expressions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclass Table (and add validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/tomkooij/scipy2017/blob/master/notebooks/02-Datatypes-in-HDF5.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soft links and hard links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column to table (like a database schema migration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undo/redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 users and job offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to go from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
